{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c4a9c63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import nltk \n",
    "import gensim\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d2a236e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mars', 'is', 'approximately', 'half', 'the', 'diameter', 'of', 'Earth', '.']\n"
     ]
    }
   ],
   "source": [
    "data = \"Mars is approximately half the diameter of Earth.\"\n",
    "print(word_tokenize(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c7bff54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mars is approximately half the diameter of Earth.', 'It is half the size of Earth.']\n"
     ]
    }
   ],
   "source": [
    "data = \"Mars is approximately half the diameter of Earth. It is half the size of Earth.\"\n",
    "print(sent_tokenize(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf68658",
   "metadata": {},
   "source": [
    "#### Open a file and tokenize sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc040649",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../src-data/test.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9396/2910112426.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfile_docs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../src-data/test.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mfile_docs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../src-data/test.txt'"
     ]
    }
   ],
   "source": [
    "file_docs = []\n",
    "with open('../src-data/test.txt', encoding='utf8') as f:\n",
    "    tokens = sent_tokenize(f.read())\n",
    "    for line in tokens:\n",
    "        file_docs.append(line)\n",
    "len(file_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1195e5",
   "metadata": {},
   "source": [
    "### Tokenize words and create dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4d42441c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is', 'the', 'space', '.'], ['this', 'is', 'our', 'planet', '.'], ['this', 'is', 'the', 'mars', '.']]\n"
     ]
    }
   ],
   "source": [
    "gen_docs = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in file_docs]\n",
    "print(gen_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d601680",
   "metadata": {},
   "source": [
    "#### Creating Disctionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b9bbb450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 0, 'is': 1, 'space': 2, 'the': 3, 'this': 4, 'our': 5, 'planet': 6, 'mars': 7}\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c700f84",
   "metadata": {},
   "source": [
    "#### Creating a bag of words (Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5d30b52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)],\n",
       " [(0, 1), (1, 1), (4, 1), (5, 1), (6, 1)],\n",
       " [(0, 1), (1, 1), (3, 1), (4, 1), (7, 1)]]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text =  'Mars is a cold desert world. It is half the size of the Earth.'\n",
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da914571",
   "metadata": {},
   "source": [
    "#### TF- IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b600f100",
   "metadata": {},
   "source": [
    "<b>TF - Term Frequency</b>- How often the word appear in a document <br>\n",
    "<b>IDF -  Inverse document frequency </b> How rarely a word appear in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fb2b0e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['space', 0.94], ['the', 0.35]]\n",
      "[['our', 0.71], ['planet', 0.71]]\n",
      "[['the', 0.35], ['mars', 0.94]]\n"
     ]
    }
   ],
   "source": [
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "for doc in tf_idf[corpus]:\n",
    "    print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b4a905f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating Similarty Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bb5f429e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.similarities.docsim.Similarity at 0x1a60aa60>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = gensim.similarities.Similarity('../src-data/test.txt',tf_idf[corpus], num_features=len(dictionary))\n",
    "sims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246f618b",
   "metadata": {},
   "source": [
    "### Create a second corpus from a  second document, Then find their similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "97314e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_doc_bow  = the gensim.models.TfidfModel(corpus) of the second document - the second document is consided to be quried to the document (corpus)\n",
    "# query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "# print('Result:', sims[query_doc_tf_idf]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299ad1c7",
   "metadata": {},
   "source": [
    "### Sum their overall similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9e350d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
    "# print(sum_of_sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27112620",
   "metadata": {},
   "source": [
    "Then average similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c145d68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage_of_similarity = round(float((sum_of_sims / len(file_docs)) * 100))\n",
    "# print(f'Average similarity float: {float(sum_of_sims / len(file_docs))}')\n",
    "# print(f'Average similarity percentage: {float(sum_of_sims / len(file_docs)) * 100}')\n",
    "# print(f'Average similarity rounded percentage: {percentage_of_similarity}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
